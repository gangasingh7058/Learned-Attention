# Learned Attention ğŸ­

A decoder-only Transformer with a **learned soft attention mask** â€” each layer dynamically predicts *what to attend to* based on the input, rather than relying solely on a fixed causal mask.

## Core Idea

Standard Transformers use a static causal mask (lower-triangular) that uniformly allows attending to all past tokens. This model adds a **learnable, data-dependent soft mask** that modulates attention weights *after* softmax. Each layer contains a `PredictMask` sub-network that:

1. Runs self-attention over the input to understand context
2. Projects to a `(B, Seq, Seq)` mask via a linear layer
3. Sharpens the mask with a scaled sigmoid
4. Multiplies it element-wise with the main attention weights (post-softmax)

This allows the model to learn **per-layer, input-dependent attention patterns** â€” selectively amplifying or suppressing token relationships.

## Architecture

```
Input â†’ Embedding (weight-tied) â†’ [ProcessWithLearnedMask Ã— N] â†’ RMSNorm â†’ Linear â†’ Logits
                                         â”‚
                                         â”œâ”€â”€ PredictMask: Self-Attn â†’ Linear â†’ Sigmoid â†’ soft_mask
                                         â”œâ”€â”€ Main Attention (RoPE + causal + soft_mask)
                                         â””â”€â”€ SwiGLU FeedForward
```

**Key components:**
- **RoPE** (Rotary Position Embeddings)
- **SwiGLU** FFN (gated variant with SiLU activation)
- **RMSNorm** (pre-norm architecture)
- **Weight tying** between embedding and output projection

## Default Configuration

| Parameter | Value |
|---|---|
| `d_model` | 256 |
| `n_layers` | 4 |
| `n_heads` | 4 |
| `max_Seq_len` | 128 |
| `vocab_size` | 50,257 (GPT-2 BPE) |
| `ffn_dim_multiplier` | 1 |
| Total Parameters | ~15.7M |

## Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model_args.py     # Model hyperparameters (dataclass)
â”‚   â”œâ”€â”€ model.py          # Transformer + PredictMask architecture
â”‚   â”œâ”€â”€ dataset.py        # Shakespeare dataset with tiktoken BPE
â”‚   â”œâ”€â”€ train.py          # Training loop (AdamW, cosine LR, checkpointing)
â”‚   â””â”€â”€ inference.py      # Autoregressive text generation
â”œâ”€â”€ Datasets/
â”‚   â””â”€â”€ Shakespear_dataset/
â””â”€â”€ checkpoints/          # Saved model checkpoints & loss histograms
```

## Quick Start

### Requirements

```bash
pip install torch tiktoken matplotlib tqdm
```

### Train

```bash
cd src
python train.py
```

Training saves checkpoints and loss distribution histograms to `checkpoints/` after every epoch.

### Generate Text

```bash
cd src
python inference.py
```

Generates text from the prompt `"To be or not to be, that is"` using the trained model.

## How the Learned Mask Works

In each Transformer layer (`ProcessWithLearnedMask`):

```
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚     PredictMask       â”‚
    x â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Self-Attn + Linear   â”‚â”€â”€â–º soft_mask (B, 1, Seq, Seq)
                     â”‚  + Scaled Sigmoid     â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
    x â”€â”€â–º RMSNorm â”€â”€â–º Multi-Head Attention â—„â”€â”€â”€ hard causal mask
                          â”‚         â–²                    
                          â”‚    soft_mask (post-softmax modulation)
                          â–¼
                     + residual
                          â”‚
                     â”€â”€â–º RMSNorm â”€â”€â–º SwiGLU FFN â”€â”€â–º + residual â”€â”€â–º output
```

The soft mask is applied **after softmax** and the attention is **re-normalized**, keeping the distribution valid while allowing differentiable modulation.

## License

MIT
